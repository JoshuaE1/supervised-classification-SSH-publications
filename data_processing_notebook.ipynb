{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse, stats\n",
    "from mlutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to true to save intermediate files\n",
    "SAVE_INTERMEDIATE_FILES = False\n",
    "# Random seed\n",
    "RANDOM_SEED = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'Abstract': str,\n",
    "    'Title': str,\n",
    "    'year': int,\n",
    "    'documentType': str,\n",
    "    'StoreId': str,\n",
    "    'disc1': str,\n",
    "    'disc2': str,\n",
    "}\n",
    "\n",
    "socab_df = pd.read_csv('Datasets/SocAbstracts.csv', dtype=dtypes)\n",
    "eric_df = pd.read_csv('Datasets/ERIC.csv', dtype=dtypes)\n",
    "econlit_df = pd.read_csv('Datasets/EconLit.csv', dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and relabeling\n",
    "\n",
    "Get clean and relabeled dataframes for each set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socab_clean = clean_df(socab_df)\n",
    "eric_clean = clean_df(eric_df)\n",
    "econlit_clean = clean_df(econlit_df)\n",
    "\n",
    "if SAVE_INTERMEDIATE_FILES:\n",
    "    socab_clean.to_csv(\"SocAbstracts_master.csv\", index=False)\n",
    "    eric_clean.to_csv(\"ERIC_master.csv\", index=False)\n",
    "    econlit_clean.to_csv(\"EconLit_master.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which columns are stored?\n",
    "socab_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([socab_clean,eric_clean,econlit_clean])\n",
    "df = df.drop(columns=['year', 'disc1_x', 'disc1_counts', 'disc2_counts'])\n",
    "\n",
    "if SAVE_INTERMEDIATE_FILES:\n",
    "    # Transform list to semicolon-separated string prior to saving\n",
    "    df['disc2_x'] = df.disc2_x.apply(lambda x: ';'.join(x))\n",
    "    df.to_csv(\"dataset.csv\", index=False)\n",
    "    # Read file and transform back to list format\n",
    "    df = pd.read_csv(\"dataset.csv\")\n",
    "    df['disc2_x'] = df.disc2_x.str.split(';')\n",
    "\n",
    "df.to_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df.Abstract.str.cat(df.Title, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have now we have the data textual data to train and test the machine learning modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the inter-indexer consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socab_eval = pd.read_excel(\"ExpertEvaluation/soc_ab_indexerconsis.xlsx\", dtype=str)\n",
    "vods = pd.read_excel(\"ExpertEvaluation/Vlaamse onderzoeksdisciplinelijst_V2018.xlsx\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value '0' represents NaN\n",
    "socab_eval = socab_eval.replace('0', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all discipline codes are in official discipline codelist / no typos\n",
    "codes = set(vods['Unnamed: 6'])\n",
    "\n",
    "print('Are all labels in the original vods codelist?')\n",
    "print('Expert labels:', all(socab_eval[f'expert_label{i}'].isin(codes).all() for i in range(1, 6)))\n",
    "print('Expected labels:', all(socab_eval[f'expected_label{i}'].isin(codes).all() for i in range(1, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create level 3 columns\n",
    "for i in range(1, 6):\n",
    "    expected, expert = f'expected_label{i}', f'expert_label{i}'\n",
    "    \n",
    "    try:\n",
    "        socab_eval[f'expected_lv3label{i}'] = socab_eval[expected][socab_eval[expected].notna()].str[:-2]\n",
    "        socab_eval[f'expert_lv3label{i}'] = socab_eval[expert][socab_eval[expert].notna()].str[:-2]\n",
    "        \n",
    "    except AttributeError:\n",
    "        socab_eval[f'expected_lv3label{i}'] = pd.Series()\n",
    "        socab_eval[f'expert_lv3label{i}'] = pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_lv4 = [c for c in socab_eval.columns if c.startswith('expected_label')]\n",
    "expert_lv4 = [c for c in socab_eval.columns if c.startswith('expert_label')]\n",
    "expected_lv3 = [c for c in socab_eval.columns if c.startswith('expected_lv3label')]\n",
    "expert_lv3 = [c for c in socab_eval.columns if c.startswith('expert_lv3label')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_without_nan(row, cols):\n",
    "    return set(row[cols][row[cols].notna()])\n",
    "\n",
    "def consistency_score(row, level):\n",
    "    if level == 4:\n",
    "        expected, expert = expected_lv4, expert_lv4\n",
    "        \n",
    "    elif level == 3:\n",
    "        expected, expert = expected_lv3, expert_lv3\n",
    "        \n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "    return (\n",
    "        2 * len(set_without_nan(row, expected) & set_without_nan(row, expert))\n",
    "        / (len(set_without_nan(row, expected)) + len(set_without_nan(row, expert)))\n",
    "    )\n",
    "\n",
    "socab_eval['consistency_lvl4'] = socab_eval.apply(consistency_score, axis=1, level=4)\n",
    "socab_eval['consistency_lvl3'] = socab_eval.apply(consistency_score, axis=1, level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inter-indexer consistency on level 3 = {}\".format(sum(socab_eval.consistency_lvl3) / len(socab_eval)))\n",
    "print(\"Inter-indexer consistency on level 4 = {}\".format(sum(socab_eval.consistency_lvl4) / len(socab_eval)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
